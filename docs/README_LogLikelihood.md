# ğŸ§  Understanding Log-Likelihood in Language Models

Language models (like **Gemma**, **GPT**, or others) are trained to predict
â€œ**what token comes next given the previous ones**.â€
When we evaluate them, we often measure how *probable* a correct answer is under the model â€” this is called **log-likelihood**.

---

## âš™ï¸ Mathematical Definition

For a given input *x* and correct answer *y = (yâ‚, yâ‚‚, ..., yâ‚œ)*(a sequence of tokens),
the probability of the answer is:

\**P(y|x) = âˆâ‚œâ‚Œâ‚áµ€ P(yâ‚œ | x, yâ‚â‚â‚œâ‚‹â‚â‚â‚)**

Taking the logarithm gives the **log-likelihood**:

\**log P(y|x) = Î£â‚œâ‚Œâ‚áµ€ log P(yâ‚œ | x, yâ‚â‚â‚œâ‚‹â‚â‚â‚)**

This value tells us how *confident* the model is in the gold answer.

---

## ğŸ’¡ Intuitive Explanation

The model never â€œseesâ€ the full answer.During evaluation, the system feeds the model the **context plus the answer up to token \(t-1\)**and asks:

> â€œIf you knew this much, how likely is the next token?â€

The model doesnâ€™t generate â€” it *evaluates* the likelihood of the correct continuation.

At each step:

- The prefix (context + previous tokens) is given to the model.
- The model predicts probabilities for *all* possible next tokens.
- We extract the probability of the **true next token**.

---

## ğŸ“Š Example Calculation

Suppose the question is:

> Ú†Ù‡ Ù‡Ù…Ú©Ø§Ø±ÛŒâ€ŒØ§ÛŒ Ø¯Ø± Ø¯Ù‡Ù‡Ù” Û±Û³Û¶Û° Ø¨Ù‡ ØªØ­ÙˆÙ„ Ø§ÛŒØ±Ø§Ù†â€ŒØ®ÙˆØ¯Ø±Ùˆ Ú©Ù…Ú© Ú©Ø±Ø¯ØŸ

and the correct answer is:

> Ø§Ø¹Ù„Ø§Ù… Ø¢Ù…Ø§Ø¯Ú¯ÛŒ Ù¾Ú˜Ùˆ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ú©Ø§Ø±ÛŒ

We break the answer into tokens and get model probabilities like this:


<div dir="rtl">

| ğŸ”¢ Ú¯Ø§Ù… | ğŸª¶ ØªÙˆÚ©Ù† | ğŸ”¢ Ø§Ø­ØªÙ…Ø§Ù„ \(P(y_t | x, y_{<t})\) | ğŸ§® Ù„Ú¯Ø§Ø±ÛŒØªÙ… Ø§Ø­ØªÙ…Ø§Ù„ \(\log P(y_t | x, y_{<t})\) |
|:--:|:---------------------------|:----------------:|:----------------:|
| Û± | Ø§Ø¹Ù„Ø§Ù… (*announcement*) | 0.25 | âˆ’1.386 |
| Û² | Ø¢Ù…Ø§Ø¯Ú¯ÛŒ (*readiness*) | 0.40 | âˆ’0.916 |
| Û³ | Ù¾Ú˜Ùˆ (*Peugeot*) | 0.10 | âˆ’2.302 |
| Û´ | Ø¨Ø±Ø§ÛŒ (*for*) | 0.30 | âˆ’1.203 |
| Ûµ | Ù‡Ù…Ú©Ø§Ø±ÛŒ (*cooperation*) | 0.50 | âˆ’0.693 |

</div>


Then:

**P(y | x) = 0.25 Ã— 0.40 Ã— 0.10 Ã— 0.30 Ã— 0.50 = 0.0015**

**log P(y | x) = âˆ’1.386 âˆ’ 0.916 âˆ’ 2.302 âˆ’ 1.203 âˆ’ 0.693 = âˆ’6.5**

So the **log-likelihood = âˆ’6.5**.

- If itâ€™s **close to zero**, the model is *confident* and likely correct.
- If itâ€™s **very negative**, the model is uncertain or wrong.

---

## ğŸ” Step-by-Step Process

| Step | Model Input                                           | Model Predicts               | We Take                 |
| ---- | ----------------------------------------------------- | ---------------------------- | ----------------------- |
| 1    | `context`                                           | probabilities for all tokens | log P(â€œØ§Ø¹Ù„Ø§Ù…â€)   |
| 2    | `context + Ø§Ø¹Ù„Ø§Ù…`                              | probabilities for all tokens | log P(â€œØ¢Ù…Ø§Ø¯Ú¯ÛŒâ€) |
| 3    | `context + Ø§Ø¹Ù„Ø§Ù… Ø¢Ù…Ø§Ø¯Ú¯ÛŒ`                 | probabilities for all tokens | log P(â€œÙ¾Ú˜Ùˆâ€)       |
| 4    | `context + Ø§Ø¹Ù„Ø§Ù… Ø¢Ù…Ø§Ø¯Ú¯ÛŒ Ù¾Ú˜Ùˆ`          | probabilities for all tokens | log P(â€œØ¨Ø±Ø§ÛŒâ€)     |
| 5    | `context + Ø§Ø¹Ù„Ø§Ù… Ø¢Ù…Ø§Ø¯Ú¯ÛŒ Ù¾Ú˜Ùˆ Ø¨Ø±Ø§ÛŒ` | probabilities for all tokens | log P(â€œÙ‡Ù…Ú©Ø§Ø±ÛŒâ€) |

This continues until all tokens in the gold answer are covered.

---

## ğŸ“ˆ Relation to Evaluation in `lm-evaluation-harness`

In evaluation tools like [`lm-evaluation-harness`](https://github.com/EleutherAI/lm-evaluation-harness),log-likelihood is used for QA tasks such as **SQuAD** to calculate:

- **Exact Match (EM)**
- **F1 score**
- Confidence-based ranking of predictions

Each record logs something like:

```json
["-33.25", "False"]
```

Meaning:

- Log-likelihood = âˆ’33.25
- Modelâ€™s predicted answer did **not** match the gold answer.

---

## ğŸ§® Summary

| Concept        | Meaning                    | Interpretation                                   |
| -------------- | -------------------------- | ------------------------------------------------ |
| \(P(y          | x)\)                       | Probability the model assigns to the gold answer |
| Log-likelihood | Sum of log P of all tokens | Measures model confidence                        |
| Closer to 0    | Model is confident         | Likely correct                                   |
| Very negative  | Model uncertain            | Likely wrong                                     |

---

## ğŸ“š References

- [EleutherAI / lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
- [Hugging Face Transformers Docs](https://huggingface.co/docs/transformers/)
- [Google Gemma Models](https://huggingface.co/google/gemma-2b-it)
